{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BGRL Tutorial\n",
    "#### This tutorial illustrates the use of BGRL (Bootstrapped Graph Latents) algorithm [Large-Scale Representation Learning on Graphs via Bootstrapping](https://arxiv.org/abs/2102.06514), a graph representation learning method that learns by predicting alternative augmentations of the input.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. Preprocessing Data and Loading Configuration\n",
    "#### 2. Training the model\n",
    "#### 3. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented BGRL is similar to the value reported in the paper. Note that we also define the random augmentations used in BGRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:09.272477400Z",
     "start_time": "2023-10-21T19:54:09.255915800Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' via  \"\n",
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from src.augment import RandomMask, RandomDropEdge, RandomDropNode, AugmentSubgraph, AugmentorList, AugmentorDict, RandomMaskChannel\n",
    "from src.methods import BGRL, BGRLEncoder\n",
    "from src.trainer import SimpleTrainer\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src.transforms import NormalizeFeatures, GCNNorm, Edge2Adj, Compose\n",
    "from src.evaluation import LogisticRegression\n",
    "from src.data.data_non_contrast import Dataset\n",
    "import torch, copy\n",
    "import numpy as np\n",
    "from src.config import load_yaml\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "config = load_yaml('./configuration/bgrl_cs.yml')\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "# WikiCS, cora, citeseer, pubmed, photo, computers, cs, and physics\n",
    "data_name = config.dataset.name\n",
    "root = config.dataset.root\n",
    "\n",
    "dataset = Dataset(root=root, name=data_name)\n",
    "if not hasattr(dataset, \"adj_t\"):\n",
    "    data = dataset.data\n",
    "    dataset.data.adj_t = torch.sparse.FloatTensor(data.edge_index, torch.ones_like(data.edge_index[0]), [data.x.shape[0], data.x.shape[0]])\n",
    "data_loader = DataLoader(dataset)\n",
    "# dataset.data.x[7028] = torch.zeros((300))\n",
    "\n",
    "# Augmentation\n",
    "augment_1 = AugmentorList([RandomDropEdge(config.model.aug_edge_1), RandomMaskChannel(config.model.aug_mask_1)])\n",
    "augment_2 = AugmentorList([RandomDropEdge(config.model.aug_edge_2), RandomMaskChannel(config.model.aug_mask_2)])\n",
    "augment = AugmentorDict({\"augment_1\":augment_1, \"augment_2\":augment_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training the Model\n",
    "#### In the second step, we first initialize the parameters of BGRL. The backbone of the encoder is GCN.\n",
    "#### Some specific hyper-parameters in the model are the augmentation ratios for the edge drop and feature mask augmentations.\n",
    "#### You may replace the encoder with the user-defined encoder. Please refer to the framework of the encoder in methods/afgrl.py. Keep in mind that the encoder consists of class initialization, forward function, and get_embs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:12.443915800Z",
     "start_time": "2023-10-21T19:54:12.438717600Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if data_name==\"cora\":\n",
    "    student_encoder = BGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[2048])\n",
    "elif data_name==\"photo\":\n",
    "    student_encoder = BGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[512, 256])\n",
    "elif data_name==\"wikics\":\n",
    "    student_encoder = BGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[512, 256])\n",
    "elif data_name==\"cs\":\n",
    "    student_encoder = BGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[256])\n",
    "\n",
    "teacher_encoder = copy.deepcopy(student_encoder)\n",
    "\n",
    "method = BGRL(student_encoder=student_encoder, teacher_encoder = teacher_encoder, data_augment=augment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### We train the model by calling the trainer.train() function. Please run the code in examples for full demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:20.347461700Z",
     "start_time": "2023-10-21T19:54:15.059680200Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Trainer --------------------\n",
    "trainer = SimpleTrainer(method=method, data_loader=data_loader, device=device, use_ema=True, \\\n",
    "                        moving_average_decay=config.optim.moving_average_decay, lr=config.optim.base_lr, \n",
    "                        weight_decay=config.optim.weight_decay, dataset=dataset, n_epochs=config.optim.max_epoch,\\\n",
    "                        patience=config.optim.patience)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Evaluating the performance of BGRL\n",
    "#### In the last step, we evaluate the performance of BGRL. We first get the embedding by calling method.get_embs() function and then use logistic regression to evaluate its performance. \n",
    "#### The more choice of classifiers can be found in [classifier.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/classifier.py), including SVM, RandomForest, etc. \n",
    "#### Besides, other evaluation methods in an unsupervised setting could be found in [cluster.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/cluster.py) or [sim_search.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/sim_search.py), including K-means method or similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:24.807636100Z",
     "start_time": "2023-10-21T19:54:23.650700200Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Evaluator -------------------\n",
    "method.eval()\n",
    "data_pyg = dataset.data.to(method.device)\n",
    "embs = method.get_embs(data_pyg, data_pyg.edge_index).detach()\n",
    "\n",
    "lg = LogisticRegression(lr=0.01, weight_decay=0, max_iter=100, n_run=20, device=device)\n",
    "lg(embs=embs, dataset=data_pyg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
