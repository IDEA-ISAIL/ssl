{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AFGRL Tutorial\n",
    "#### This tutorial illustrates the use of AFGRL algorithm [Augmentation-Free Self-Supervised Learning on Graphs](https://arxiv.org/abs/2112.02472), an augmentation-free self-supervised learning framework, which generates an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. Preprocessing Data and Loading Configuration\n",
    "#### 2. Training the model\n",
    "#### 3. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented AFGRL is similar to the value reported in the paper. Note here we also load graph augmentor in AFGRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:09.272477400Z",
     "start_time": "2023-10-21T19:54:09.255915800Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' via  \"\n",
      "/home/twei10/anaconda3/envs/ssl/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from src.augment import RandomMask, RandomDropEdge, RandomDropNode, AugmentSubgraph, AugmentorList, AugmentorDict, NeighborSearch_AFGRL\n",
    "from src.methods import AFGRLEncoder, AFGRL\n",
    "from src.trainer import SimpleTrainer\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src.transforms import NormalizeFeatures, GCNNorm, Edge2Adj, Compose\n",
    "from src.datasets import Planetoid, Amazon, WikiCS\n",
    "from src.evaluation import LogisticRegression\n",
    "import torch, copy\n",
    "import torch_geometric\n",
    "from src.data.utils import sparse_mx_to_torch_sparse_tensor\n",
    "from src.data.data_non_contrast import Dataset\n",
    "import numpy as np\n",
    "from src.config import load_yaml\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "config = load_yaml('./configuration/afgrl_cs.yml')\n",
    "# config = load_yaml('./configuration/afgrl_wikics.yml')\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "\n",
    "# WikiCS, cora, citeseer, pubmed, photo, computers, cs, and physics\n",
    "data_name = config.dataset.name\n",
    "root = config.dataset.root\n",
    "\n",
    "dataset = Dataset(root=root, name=data_name)\n",
    "if not hasattr(dataset, \"adj_t\"):\n",
    "    data = dataset.data\n",
    "    dataset.data.adj_t = torch.sparse.FloatTensor(data.edge_index, torch.ones_like(data.edge_index[0]), [data.x.shape[0], data.x.shape[0]])\n",
    "data_loader = DataLoader(dataset)\n",
    "data = dataset.data\n",
    "# data.x[7028] = torch.zeros((300))\n",
    "adj_ori_sparse = torch.sparse.FloatTensor(data.edge_index, torch.ones_like(data.edge_index[0]), [data.x.shape[0], data.x.shape[0]]).to(device)\n",
    "# Augmentation\n",
    "augment = NeighborSearch_AFGRL(device=device, num_centroids=config.model.num_centroids, num_kmeans=config.model.num_kmeans, clus_num_iters=config.model.clus_num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training the Model\n",
    "#### In the second step, we first initialize the parameters of AFGRL. The backbone of the encoder is GCN.\n",
    "#### Some specific hyper-parameters in the model includes, topk: the number of neighbors for nearest neighborhood search, num_centroids: number of centroids in K-means Clustering of the augmentor, num_kmeans: the number of iterations for K-means Clustering.\n",
    "#### You may replace the encoder with the user-defined encoder. Please refer to the framework of the encoder in methods/afgrl.py. Keep in mind that the encoder consists of class initialization, forward function, and get_embs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:12.443915800Z",
     "start_time": "2023-10-21T19:54:12.438717600Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Method -----------------\n",
    "if data_name==\"cora\":\n",
    "    student_encoder = AFGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[2048])\n",
    "elif data_name==\"photo\":\n",
    "    student_encoder = AFGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[512, 512])\n",
    "elif data_name==\"wikics\":\n",
    "    student_encoder = AFGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[512, 256])\n",
    "elif data_name==\"cs\":\n",
    "    student_encoder = AFGRLEncoder(in_channel=dataset.x.shape[1], hidden_channels=[512, 256])\n",
    "teacher_encoder = copy.deepcopy(student_encoder)\n",
    "\n",
    "method = AFGRL(student_encoder=student_encoder, teacher_encoder = teacher_encoder, data_augment=augment, adj_ori = adj_ori_sparse, topk=config.model.topk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### We train the model by calling the trainer.train() function. Please run the code in examples for full demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:20.347461700Z",
     "start_time": "2023-10-21T19:54:15.059680200Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Trainer --------------------\n",
    "trainer = SimpleTrainer(method=method, data_loader=data_loader, device=device, use_ema=True, \n",
    "                        moving_average_decay=config.optim.moving_average_decay, lr=config.optim.base_lr, \n",
    "                        weight_decay=config.optim.weight_decay, n_epochs=config.optim.max_epoch, dataset=dataset,\n",
    "                        patience=config.optim.patience)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Evaluating the performance of AFGRL\n",
    "#### In the last step, we evaluate the performance of AFGRL. We first get the embedding by calling method.get_embs() function and then use logistic regression to evaluate its performance. \n",
    "#### The more choice of classifiers can be found in [classifier.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/classifier.py), including SVM, RandomForest, etc. \n",
    "#### Besides, other evaluation methods in an unsupervised setting could be found in [cluster.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/cluster.py) or [sim_search.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/sim_search.py), including K-means method or similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:24.807636100Z",
     "start_time": "2023-10-21T19:54:23.650700200Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Evaluator -------------------\n",
    "method.eval()\n",
    "data_pyg = dataset.data.to(method.device)\n",
    "embs = method.get_embs(data_pyg, data_pyg.edge_index).detach()\n",
    "\n",
    "lg = LogisticRegression(lr=0.01, weight_decay=0, max_iter=100, n_run=20, device=device)\n",
    "lg(embs=embs, dataset=data_pyg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
