{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GraphCL Tutorial\n",
    "#### This tutorial illustrates the use of GraphCL algorithm ([Graph Contrastive Learning with Augmentations](https://proceedings.nips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf)), agraph contrastive learning framework for learning unsupervised representations of graph data,  which maximizes the mutual information between two augmented graphs.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. [Preprocessing Data and Loading Configuration](GraphCL.ipynb#L6)\n",
    "#### 2. [Graph Augmentation](GraphCL.ipynb#L7)\n",
    "#### 3. [Training the model](GraphCL.ipynb#L7)\n",
    "#### 4. [Evaluating the model](GraphCL.ipynb#L8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented GraphCL is similar to the value reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T00:54:47.689755100Z",
     "start_time": "2023-10-30T00:54:43.141304500Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.augment import *\n",
    "from src.methods import GraphCL, GraphCLEncoder\n",
    "from src.trainer import SimpleTrainer\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src.transforms import NormalizeFeatures, GCNNorm, Edge2Adj, Compose\n",
    "from src.datasets import Planetoid, Entities, Amazon, WikiCS, Coauthor\n",
    "from src.evaluation import LogisticRegression\n",
    "import torch\n",
    "from src.config import load_yaml\n",
    "from src.utils.create_data import create_masks\n",
    "from src.utils.add_adj import add_adj_t\n",
    "\n",
    "# load the configuration file\n",
    "# config = load_yaml('./configuration/graphcl_amazon.yml')\n",
    "# config = load_yaml('./configuration/graphcl_coauthor.yml')\n",
    "config = load_yaml('./configuration/graphcl_wikics.yml')\n",
    "# config = load_yaml('./configuration/graphcl_cora.yml')\n",
    "torch.manual_seed(config.torch_seed)\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "\n",
    "# data\n",
    "\n",
    "if config.dataset.name == 'pyg_data':\n",
    "    pre_transforms = Compose([NormalizeFeatures(ord=1), Edge2Adj(norm=GCNNorm(add_self_loops=1))])\n",
    "    # dataset = Planetoid(root=config.dataset.root, name=config.dataset.name, pre_transform=pre_transforms)\n",
    "    dataset = Planetoid(root='pyg_data', name=config.dataset.name)\n",
    "elif config.dataset.name == 'Amazon':\n",
    "    pre_transforms = NormalizeFeatures(ord=1)\n",
    "    dataset = Amazon(root='pyg_data', name='Photo', pre_transform=pre_transforms)\n",
    "elif config.dataset.name == 'WikiCS':\n",
    "    pre_transforms = NormalizeFeatures(ord=1)\n",
    "    dataset = WikiCS(root='pyg_data', pre_transform=pre_transforms)\n",
    "elif config.dataset.name == 'coauthor':\n",
    "    pre_transforms = NormalizeFeatures(ord=1)\n",
    "    dataset = Coauthor(root='pyg_data', name='CS', pre_transform=pre_transforms)\n",
    "else:\n",
    "    raise 'please specify the correct dataset root'\n",
    "if config.dataset.name in ['Amazon', 'WikiCS', 'coauthor']:\n",
    "    dataset.data = create_masks(dataset.data, config.dataset.name)\n",
    "dataset = add_adj_t(dataset)\n",
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Graph Augmentation\n",
    "#### GraphCL list several graph agumentation methods, ranging from the node level to edge level and subgraph level.\n",
    "#### You may change the aug_type to select different graph augmentation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T00:54:57.316076Z",
     "start_time": "2023-10-30T00:54:57.284594Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_type = config.model.aug_type\n",
    "if aug_type == 'edge':\n",
    "    augment_neg = AugmentorList([RandomDropEdge()])\n",
    "elif aug_type == 'mask':\n",
    "    augment_neg = AugmentorList([RandomMask()])\n",
    "elif aug_type == 'node':\n",
    "    augment_neg = AugmentorList([RandomDropNode()])\n",
    "elif aug_type == 'subgraph':\n",
    "    augment_neg = AugmentorList([AugmentSubgraph()])\n",
    "else:\n",
    "    assert 'unrecognized augmentation method'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Training the Model\n",
    "#### In the second step, we first initialize the parameters of GraphCL. The backbone of GraphCL is Deep InfoMax (DGI). \n",
    "#### You may replace the encoder with the user-defined encoder. Please refer to the framework of encoder in the directory (./src/methods/graphcl.py#L96). Keep in mind that the encoder consists of class initialization, forward function and get_embs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T00:55:11.505655200Z",
     "start_time": "2023-10-30T00:55:11.498174400Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Method -----------------\n",
    "encoder = GraphCLEncoder(in_channels=config.model.in_channels, hidden_channels=config.model.hidden_channels)\n",
    "method = GraphCL(encoder=encoder, corruption=augment_neg, hidden_channels=config.model.hidden_channels)\n",
    "method.augment_type = aug_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### We train the model by calling trainer.train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T00:55:43.762452300Z",
     "start_time": "2023-10-30T00:55:14.091375700Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SimpleTrainer(method=method, data_loader=data_loader, device=device, n_epochs=config.optim.max_epoch)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Evaluating the performance of GraphCL\n",
    "#### In the last step, we evaluate the performance of GraphCL. We first get the embedding of by calling method.get_embs() function and then we use logistic regression to evaluate its performance. The more choice of classifier could be found in the directory (./src/evaluation/classifier.py), including svm, randomforest, etc. Besides, other evaluation methods in unsupervised setting could be found in the directory (./src/evaluation/cluster.py or ./src/evaluation/sim_search.py), including kmean method or similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T00:56:36.012300Z",
     "start_time": "2023-10-30T00:55:58.859115800Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Evaluator -------------------\n",
    "data_pyg = dataset.data.to(method.device)\n",
    "embs = method.get_embs(data_pyg, data_pyg.adj_t).detach()\n",
    "\n",
    "lg = LogisticRegression(lr=config.classifier.base_lr, weight_decay=config.classifier.weight_decay,\n",
    "                        max_iter=config.classifier.max_epoch, n_run=1, device=device)\n",
    "lg(embs=embs, dataset=data_pyg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
