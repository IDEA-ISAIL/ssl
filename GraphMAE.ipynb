{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GraphMAE Tutorial\n",
    "#### This tutorial illustrates the use of GraphMAE algorithm [GraphMAE:Self-SupervisedMaskedGraphAutoencoders](https://arxiv.org/pdf/2205.10803.pdf), a masked graph autoencoder method for self-supervised graph representation learning. It focuses on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. [Preprocessing Data and Loading Configuration](GraphMAE.ipynb#L6)\n",
    "#### 2. [Training the model](GraphMAE.ipynb#L7)\n",
    "#### 3. [Evaluating the model](GraphMAE.ipynb#L8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented GraphMAE is similar to the value reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T06:06:23.833718900Z",
     "start_time": "2024-02-29T06:06:23.682017600Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.methods.graphmae import GraphMAE, EncoderDecoder, load_graph_classification_dataset, setup_loss_fn, collate_fn\n",
    "from src.trainer import SimpleTrainer\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.config import load_yaml\n",
    "import os\n",
    "from src.evaluation import LogisticRegression\n",
    "\n",
    "config = load_yaml('./configuration/graphmae_mutag.yml')\n",
    "# config = load_yaml('./configuration/graphmae_imdb_b.yml')\n",
    "# config = load_yaml('./configuration/graphmae_imdb_m.yml')\n",
    "torch.manual_seed(config.torch_seed)\n",
    "np.random.seed(config.torch_seed)\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "\n",
    "current_folder = os.path.abspath('')\n",
    "# path = os.path.join(os.path.dirname(os.path.realpath(__file__)), config.dataset.root, config.dataset.name)\n",
    "path = os.path.join(os.path.dirname(os.path.abspath('')), config.dataset.root, config.dataset.name)\n",
    "\n",
    "# -------------------- Data --------------------\n",
    "dataset, num_features = load_graph_classification_dataset(config.dataset.name, raw_dir=path)\n",
    "train_idx = torch.arange(len(dataset))\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "eval_loader = GraphDataLoader(dataset, collate_fn=collate_fn, batch_size=config.dataset.batch_size, shuffle=False)\n",
    "in_channels = max(num_features, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training the Model\n",
    "#### In the second step, we first initialize the parameters of GraphMAE. The backbone of the encoder is Graph Isomorphism Network (GIN), while you may change the encoder type to other GNNs, such as 'gat', 'dotgat', 'GCN' or 'MLP' (2-layer MLP). \n",
    "#### You may replace the encoder with the user-defined encoder. Please refer to the framework of encoder in the directory (./src/methods/graphmae.py#L16). Keep in mind that the encoder consists of class initialization and forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T06:06:23.977599800Z",
     "start_time": "2024-02-29T06:06:23.833718900Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Method -----------------\n",
    "pooling = config.model.pooling\n",
    "if pooling == \"mean\":\n",
    "    pooler = AvgPooling()\n",
    "elif pooling == \"max\":\n",
    "    pooler = MaxPooling()\n",
    "elif pooling == \"sum\":\n",
    "    pooler = SumPooling()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "encoder = EncoderDecoder(GNN=config.model.encoder_type, enc_dec=\"encoding\", in_channels=in_channels,\n",
    "                         hidden_channels=config.model.hidden_channels, num_layers=config.model.encoder_layers)\n",
    "decoder = EncoderDecoder(GNN=config.model.decoder_type, enc_dec=\"decoding\", in_channels=config.model.hidden_channels,\n",
    "                         hidden_channels=in_channels, num_layers=config.model.decoder_layers)\n",
    "loss_function = setup_loss_fn(config.model.loss_fn, alpha_l=config.model.alpha_l)\n",
    "method = GraphMAE(encoder=encoder, decoder=decoder, hidden_channels=config.model.hidden_channels, argument=config, loss_function=loss_function)\n",
    "method.device = device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### We train the model by calling trainer.train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T06:06:35.921965600Z",
     "start_time": "2024-02-29T06:06:23.979691900Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Trainer --------------------\n",
    "trainer = SimpleTrainer(method=method, data_loader=dataset, device=device, n_epochs=config.optim.max_epoch,\n",
    "                        lr=config.optim.base_lr)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Evaluating the performance of GraphMAE\n",
    "#### In the last step, we evaluate the performance of GraphMAE. We first get the embedding of by calling method.get_embeddings() function and then we use logistic regression to evaluate its performance. The more choice of classifier could be found in the directory (./src/evaluation/classifier.py), including svm, randomforest, etc. Besides, other evaluation methods in unsupervised setting could be found in the directory (./src/evaluation/cluster.py or ./src/evaluation/sim_search.py), including kmean method or similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T06:06:44.579143500Z",
     "start_time": "2024-02-29T06:06:35.921965600Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------ Evaluation -------------------\n",
    "x, y = method.get_embeddings(pooler, eval_loader)\n",
    "y = y.reshape(-1, )\n",
    "eval_loader.y = torch.tensor(y).long()\n",
    "eval_loader.x = torch.tensor(x)\n",
    "lg = LogisticRegression(lr=config.classifier.base_lr, weight_decay=config.classifier.weight_decay,\n",
    "                        max_iter=config.classifier.max_epoch, n_run=1, device=device)\n",
    "lg(embs=torch.tensor(x), dataset=eval_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
