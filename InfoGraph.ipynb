{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# InfoGraph Tutorial\n",
    "#### This tutorial illustrates the use of InfoGraph algorithm [InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization](https://openreview.net/pdf?id=r1lfF2NYvH), an unsupervised and semisupervised graph-level representation learning method,  which maximizes the mutual information between the graph-level representation and the representations of substructures of different scales.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. [Preprocessing Data and Loading Configuration](InfoGraph.ipynb#L6)\n",
    "#### 2. [Training the model](InfoGraph.ipynb#L7)\n",
    "#### 3. [Evaluating the model](InfoGraph.ipynb#L8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented InfoGraph is similar to the value reported in the paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from src.methods.infograph import InfoGraph, Encoder\n",
    "from src.trainer import SimpleTrainer\n",
    "from src.evaluation import LogisticRegression\n",
    "from torch_geometric.datasets import TUDataset, Entities\n",
    "import os\n",
    "from torch_geometric.nn import GINConv\n",
    "from src.config import load_yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "config = load_yaml('./configuration/infograph_mutag.yml')\n",
    "# config = load_yaml('./configuration/infograph_imdb_b.yml')\n",
    "# config = load_yaml('./configuration/infograph_imdb_m.yml')\n",
    "torch.manual_seed(config.torch_seed)\n",
    "np.random.seed(config.torch_seed)\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "\n",
    "# -------------------- Data --------------------\n",
    "current_folder = os.path.abspath('')\n",
    "path = os.path.join(current_folder, config.dataset.root, config.dataset.name)\n",
    "if config.dataset.name in ['IMDB-B', 'IMDB-M', 'mutag', 'COLLAB', 'PROTEINS']:\n",
    "    # dataset = TUDataset(path, name=config.dataset.name).shuffle()\n",
    "    dataset = TUDataset(path, name=config.dataset.name)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "# dataset.x = torch.rand(dataset.y.shape[0], 100)\n",
    "data_loader = DataLoader(dataset, batch_size=config.dataset.batch_size)\n",
    "\n",
    "in_channels = max(dataset.num_features, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-03-24T14:36:33.333050900Z",
     "start_time": "2024-03-24T14:36:27.294510900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Training the Model\n",
    "#### In the second step, we first initialize the parameters of InfoGraph. The backbone of the encoder is Graph Isomorphism Network (GIN), while InfoGraph adopts the idea of Deep InfoMax as one major loss term. \n",
    "#### You may replace the encoder with the user-defined encoder. Please refer to the framework of encoder in the directory (./src/methods/infograph.py#L96). Keep in mind that the encoder consists of class initialization, forward function and get_embs() function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ------------------- Method -----------------\n",
    "encoder = Encoder(in_channels=in_channels, hidden_channels=config.model.hidden_channels,\n",
    "                  num_layers=config.model.n_layers, GNN=GINConv)\n",
    "method = InfoGraph(encoder=encoder, hidden_channels=config.model.hidden_channels, num_layers=config.model.n_layers,\n",
    "                   prior=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-03-24T14:36:38.157089500Z",
     "start_time": "2024-03-24T14:36:38.132908300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We train the model by calling trainer.train() function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 39.8438, time: 0.5257s\n",
      "Epoch 1: loss: 29.4094, time: 0.1775s\n",
      "Epoch 2: loss: 21.1208, time: 0.1865s\n",
      "Epoch 3: loss: 15.9829, time: 0.1893s\n",
      "Epoch 4: loss: 12.7193, time: 0.1893s\n",
      "Epoch 5: loss: 12.2772, time: 0.1923s\n",
      "Epoch 6: loss: 10.2991, time: 0.1923s\n",
      "Epoch 7: loss: 8.8431, time: 0.1824s\n",
      "Epoch 8: loss: 7.1507, time: 0.2012s\n",
      "Epoch 9: loss: 6.6997, time: 0.1932s\n",
      "Epoch 10: loss: 5.6720, time: 0.1839s\n",
      "Epoch 11: loss: 4.7716, time: 0.1793s\n",
      "Epoch 12: loss: 4.6085, time: 0.1904s\n",
      "Epoch 13: loss: 3.9663, time: 0.1873s\n",
      "Epoch 14: loss: 3.6102, time: 0.1889s\n",
      "Epoch 15: loss: 3.2027, time: 0.1887s\n",
      "Epoch 16: loss: 3.3492, time: 0.1969s\n",
      "Epoch 17: loss: 3.3818, time: 0.1840s\n",
      "Epoch 18: loss: 2.9877, time: 0.1868s\n",
      "Epoch 19: loss: 2.2008, time: 0.1965s\n"
     ]
    }
   ],
   "source": [
    "trainer = SimpleTrainer(method=method, data_loader=data_loader, device=device, n_epochs=config.optim.max_epoch)\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-03-24T14:36:45.996578300Z",
     "start_time": "2024-03-24T14:36:41.725215800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Evaluating the performance of InfoGraph\n",
    "#### In the last step, we evaluate the performance of InfoGraph. We first get the embedding of by calling method.get_embs() function and then we use logistic regression to evaluate its performance. The more choice of classifier could be found in the directory (./src/evaluation/classifier.py), including svm, randomforest, etc. Besides, other evaluation methods in unsupervised setting could be found in the directory (./src/evaluation/cluster.py or ./src/evaluation/sim_search.py), including kmean method or similarity search."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheng\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate node classification results\n",
      "** Val: 87.8070 (5.7418) | Test: 87.8070 (5.7418) **\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Evaluator -------------------\n",
    "method.eval()\n",
    "data_pyg = dataset.data.to(method.device)\n",
    "y, embs = method.get_embs(data_loader)\n",
    "\n",
    "data_pyg.x = embs\n",
    "lg = LogisticRegression(lr=config.classifier.base_lr, weight_decay=config.classifier.weight_decay,\n",
    "                        max_iter=config.classifier.max_epoch, n_run=1, device=device)\n",
    "lg(embs=embs, dataset=data_pyg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-03-24T14:36:54.803971400Z",
     "start_time": "2024-03-24T14:36:52.351217400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
