{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Merit Tutorial\n",
    "#### This tutorial illustrates the use of Merit algorithm [Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2105.05682), an unsupervised and semisupervised graph-level representation learning method,  which maximizes the mutual information between the graph-level representation and the representations of substructures of different scales.\n",
    "#### The tutorial is organized as folows:\n",
    "#### 1. [Preprocessing Data and Loading Configuration](InfoGraph.ipynb#L48)\n",
    "#### 2. [Training the model](InfoGraph.ipynb#L100)\n",
    "#### 3. [Evaluating the model](InfoGraph.ipynb#L206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preprocessing Data and Loading Configuration \n",
    "#### First, we load the configuration from yml file and the dataset. \n",
    "#### For easy usage, we conduct experiments to search for the best parameter across three datasets and find the proper value of parameters such that the performance of implemented InfoGraph is similar to the value reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from src.methods.merit import Merit, GCN\n",
    "from src.trainer import SimpleTrainer\n",
    "from src.evaluation import LogisticRegression\n",
    "import torch_geometric.transforms as T\n",
    "from src.transforms import NormalizeFeatures, GCNNorm, Edge2Adj, Compose\n",
    "from src.datasets import Planetoid, Amazon, WikiCS,Coauthor\n",
    "from src.utils.create_data import create_masks\n",
    "from src.evaluation import LogisticRegression\n",
    "import torch \n",
    "import yaml\n",
    "from src.utils.add_adj import add_adj_t\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "# from src.config import load_yaml\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:09.272477400Z",
     "start_time": "2023-10-21T19:54:09.255915800Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'wikics', 'torch_seed': 0, 'drop_edge': 0.4, 'drop_feat1': 0.4, 'drop_feat2': 0.4, 'projection_size': 512, 'prediction_size': 512, 'prediction_hidden_size': 4096, 'projection_hidden_size': 4096, 'beta': 0.6, 'momentum': 0.8, 'alpha': 0.05, 'sample_size': 2000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhe33/anaconda3/lib/python3.9/site-packages/torch_geometric/datasets/wikics.py:38: UserWarning: The WikiCS dataset now returns an undirected graph by default. Please explicitly specify 'is_undirected=False' to restore the old behaviour.\n",
      "  warnings.warn(\n",
      "Downloading https://github.com/pmernyei/wiki-cs-dataset/raw/master/dataset/data.json\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "config = yaml.safe_load(open(\"./configuration/merit.yml\", 'r', encoding='utf-8').read())\n",
    "print(config)\n",
    "torch.manual_seed(0)\n",
    "# np.random.seed(config.torch_seed)\n",
    "# device = torch.device(\"cuda:{}\".format(config.gpu_idx) if torch.cuda.is_available() and config.use_cuda else \"cpu\")\n",
    "\n",
    "# -------------------- Data --------------------\n",
    "pre_transforms = Compose([NormalizeFeatures(ord=1), Edge2Adj(norm=GCNNorm(add_self_loops=1))])\n",
    "data_name = config['dataset']\n",
    "\n",
    "current_folder = os.path.abspath('')\n",
    "# path = os.path.join(current_folder, config.dataset.root, config.dataset.name)\n",
    "\n",
    "if data_name==\"photo\": #91.4101\n",
    "    dataset = Amazon(root=\"pyg_data\", name=\"photo\", pre_transform=pre_transforms) \n",
    "elif data_name==\"coauthor\": # 92.0973\n",
    "    dataset = Coauthor(root=\"pyg_data\", name='cs', transform=pre_transforms)\n",
    "elif data_name==\"wikics\": #82.0109\n",
    "    dataset = WikiCS(root=\"pyg_data\", transform=T.NormalizeFeatures())\n",
    "    dataset = add_adj_t(dataset)\n",
    "    nan_mask = torch.isnan(dataset[0].x)\n",
    "    imputer = SimpleImputer()\n",
    "    dataset[0].x = torch.tensor(imputer.fit_transform(dataset[0].x))\n",
    "\n",
    "# dataset = Amazon(root=\"pyg_data\", name=\"photo\", pre_transform=pre_transforms)\n",
    "data_loader = DataLoader(dataset)\n",
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training the Model\n",
    "#### In the second step, we first initialize the parameters of Merit. The backbone of the encoder is Graph Convolutional Network (GCN). \n",
    "#### You may replace the encoder with the user-defined encoder. Keep in mind that the encoder consists of class initialization, forward function, and get_embs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:12.443915800Z",
     "start_time": "2023-10-21T19:54:12.438717600Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Method -----------------\n",
    "encoder = GCN(in_ft=data.x.shape[1], out_ft=512, projection_hidden_size=config[\"projection_hidden_size\"],\n",
    "                  projection_size=config[\"projection_size\"])\n",
    "method = Merit(encoder=encoder, data = data, config=config,device=\"cuda:0\",is_sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### We train the model by calling the trainer.train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T19:54:20.347461700Z",
     "start_time": "2023-10-21T19:54:15.059680200Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhe33/anaconda3/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:347: SparseEfficiencyWarning: splu converted its input to CSC format\n",
      "  warn('splu converted its input to CSC format', SparseEfficiencyWarning)\n",
      "/home/xhe33/anaconda3/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:239: SparseEfficiencyWarning: spsolve is more efficient when sparse b is in the CSC matrix format\n",
      "  warn('spsolve is more efficient when sparse b '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 7.9260, time: 891.0707s\n",
      "Epoch 1: loss: 7.7113, time: 889.0046s\n",
      "Epoch 2: loss: 7.6641, time: 880.7199s\n",
      "Epoch 3: loss: 7.5954, time: 853.4735s\n"
     ]
    }
   ],
   "source": [
    "trainer = SimpleTrainer(method=method, data_loader=data_loader, device=\"cuda:0\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Evaluating the performance of InfoGraph\n",
    "#### In the last step, we evaluate the performance of Merit. We first get the embedding by calling method.get_embs() function and then use logistic regression to evaluate its performance. \n",
    "#### The more choice of classifiers can be found in [classifier.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/classifier.py), including SVM, RandomForest, etc. \n",
    "#### Besides, other evaluation methods in an unsupervised setting could be found in [cluster.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/cluster.py) or [sim_search.py](https://github.com/IDEA-ISAIL/ssl/edit/molecure/src/evaluation/sim_search.py), including K-means method or similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Evaluator -------------------\n",
    "data_pyg = dataset.data.to(method.device)\n",
    "embs = method.get_embs(data_pyg, data_pyg.adj_t).detach()\n",
    "\n",
    "lg = LogisticRegression(lr=0.01, weight_decay=0, max_iter=2000, n_run=50, device=\"cuda\")\n",
    "create_masks(data=data_pyg.cpu())\n",
    "lg(embs=embs, dataset=data_pyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
